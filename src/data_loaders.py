"""
Some code below is modified from:
https://github.com/aws-samples/lm-gvp
Wang, Z., Combs, S.A., Brand, R. et al. LM-GVP: an extensible sequence and structure informed deep learning framework for protein property prediction.
Sci Rep 12, 6832 (2022). https://doi.org/10.1038/s41598-022-10775-y
"""

# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: MIT-0

"""
Util functions for loading datasets.
"""
import os
import json
import numpy as np
import torch
from transformers import BertTokenizer

from utils import prep_seq
from datasets import (
    SequenceDatasetWithTarget,
    ProteinGraphDatasetWithTarget,
    BertProteinGraphDatasetWithTarget,
)

gvp_data_dir = "data/"
def load_gvp_data(
    gvp_data_dir,
    split,
    dataset_name,
    normalize
):
    """For GVP models only.
    These prepared graph data files are generated by `generate_gvp_dataset.py`

    Args:
        task: choose from ['protease/with_tags', 'Fluorescence', 'DeepFRI_GO']
        seq_only: retain only the sequences in the returned list of objects
        split: String. Split of the dataset to be loaded. One of ['train', 'valid', 'test'].
        seq_only: Bool. Wheather or not to return only sequences without coordinates.

    Retrun:
        Dictionary containing the GVP dataset of proteins.
    """
    filename = os.path.join(gvp_data_dir, f"jsons/proteins_{split}_{dataset_name}.json")
    dataset = json.load(open(filename, "rb"))
    if normalize:
        targets = [dic['target'] for dic in dataset]
        mu = sum(targets)/len(targets)
        sigma = np.sqrt(np.var(targets))
        for dic in dataset:
            dic['target'] = (dic['target'] - mu)/sigma
    return dataset


def preprocess_seqs(tokenizer, dataset, chains):
    """Preprocess seq in dataset and bind the input_ids, attention_mask.

    Args:
        tokenizer: hugging face artifact. Tokenization to be used in the sequence.
        dataset: Dictionary containing the GVP dataset of proteins.

    Return:
        Input dataset with `input_ids` and `attention_mask`
    """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    if chains == 'both':
        heavy, light = [rec['heavy'] for rec in dataset], [rec['light'] for rec in dataset]
        encodings = [(h,l) for h,l in zip(tokenizer(heavy, pad=True, device=device), tokenizer(light, pad=True, device=device))]
    else:
        seqs = [rec['seq'] for rec in dataset]
        encodings = tokenizer(seqs, pad=True, device=device)
    pad_token_id = 21
    # add input_ids, attention_mask to the json records
    for i, rec in enumerate(dataset):
        tokens = encodings[i]
        rec["input_ids"] = tokens
        if chains == 'both':
            rec["attention_mask"] = torch.zeros(*tokens[0].shape, device=device).masked_fill(tokens[0]==pad_token_id,1), torch.zeros(*tokens[1].shape, device=device).masked_fill(tokens[1]==pad_token_id,1)
        else:
            rec["attention_mask"] = torch.zeros(*tokens.shape, device=device).masked_fill(tokens == pad_token_id, 1)
    return dataset


def get_dataset(dataset_name, chains, split, tokenizer, top_k, normalize, mifst = False):
    """Load data from files, then transform into appropriate
    Dataset objects.
    Args:
        split: one of ['train', 'valid', 'test']
        tokenizer: tokenizer to be used to encode sequences

    Return:
        Torch dataset.
    """
    dataset = load_gvp_data("data/", split=split, dataset_name=dataset_name, normalize = normalize)
    dataset = preprocess_seqs(tokenizer, dataset, chains)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    dataset = BertProteinGraphDatasetWithTarget(dataset, preprocess=True, device='cpu', top_k=top_k, mifst=mifst)
    dataset.pos_weights = None
    return dataset
